{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "t02_Optimizers.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx3siYwb0cnp"
      },
      "source": [
        "# Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-jQp4F40cnq"
      },
      "source": [
        "## Momentum\n",
        "\n",
        "One disadvantage of the SGD method is that its update direction depends entirely on the current batch, so its update is very unstable. A simple way to solve this problem is to introduce momentum.\n",
        "\n",
        "**Momentum is momentum**, which simulates the inertia of an object when it is moving, that is, the direction of the previous update is retained to a certain extent during the update, while the current update gradient is used to fine-tune the final update direction. In this way, you can increase the stability to a certain extent, so that you can learn faster, and also have the ability to get rid of local optimization.\n",
        "\n",
        " ![alt](imgo/sgd1.png)\n",
        " \n",
        " **<center>Figure :- SGD without Momentum &&&  SGD with Momentum</center>**\n",
        " \n",
        " \n",
        " ### Algorithm - \n",
        " \n",
        " $1.\\ \\textbf{m} \\leftarrow \\beta \\textbf{m} + \\eta \\triangledown_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})$\n",
        " \n",
        " $2.\\ \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\textbf{m}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zg6B0d70cnq"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhJqRxLR0cnr"
      },
      "source": [
        "## NAG\n",
        "\n",
        "### Algorithm -\n",
        "\n",
        " \n",
        " $1.\\ \\textbf{m} \\leftarrow \\beta \\textbf{m} + \\eta \\triangledown_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta} - \\beta \\textbf{m})$\n",
        " \n",
        " $2.\\ \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\textbf{m}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndpcc7Do0cnr"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0ow7M_i0cnr"
      },
      "source": [
        "## Adagrad\n",
        "\n",
        "Adagrad is an algorithm for gradient-based optimization which adapts the learning rate to the parameters, using low learning rates for parameters associated with frequently occurring features, and using high learning rates for parameters associated with infrequent features. \n",
        "\n",
        "So, it is well-suited for dealing with sparse data.\n",
        "\n",
        "But the same update rate may not be suitable for all parameters. For example, some parameters may have reached the stage where only fine-tuning is needed, but some parameters need to be adjusted a lot due to the small number of corresponding samples.\n",
        "\n",
        "Adagrad proposed this problem, an algorithm that adaptively assigns different learning rates to various parameters among them. The implication is that for each parameter, as its total distance updated increases, its learning rate also slows.\n",
        "\n",
        ">**GloVe word embedding uses adagrad where infrequent words required a greater update and frequent words require smaller updates.**\n",
        "\n",
        ">**Adagrad eliminates the need to manually tune the learning rate.**\n",
        "\n",
        "\n",
        "### Algorithm \n",
        "\n",
        "1. $ \\textbf{s} \\leftarrow  \\textbf{s} +  \\triangledown_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\otimes  \\triangledown_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})$\n",
        "\n",
        "2. $ \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}- \\eta \\triangledown_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})   \\oslash \\sqrt{\\textbf{s} + \\epsilon}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b7cDNvU0cns"
      },
      "source": [
        "optimizer = keras.optimizers.Adagrad(lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NnvP4JR0cns"
      },
      "source": [
        "## RMSProp\n",
        "\n",
        "The full name of RMSProp algorithm is called **Root Mean Square Prop**, which is an adaptive learning rate optimization algorithm proposed by Geoff Hinton. \n",
        "\n",
        "\n",
        ">RMSProp tries to resolve Adagradâ€™s radically diminishing learning rates by using a moving average of the squared gradient. It utilizes the magnitude of the recent gradient descents to normalize the gradient.\n",
        "\n",
        "\n",
        "Adagrad will accumulate all previous gradient squares, and RMSprop just calculates the corresponding average value, so it can alleviate the problem that the learning rate of the Adagrad algorithm drops quickly.\n",
        "\n",
        "The difference is that RMSProp calculates the **differential squared weighted average of the gradient** . This method is beneficial to eliminate the direction of large swing amplitude, and is used to correct the swing amplitude, so that the swing amplitude in each dimension is smaller. On the other hand, it also makes the network function converge faster. \n",
        "\n",
        "\n",
        ">In RMSProp learning rate gets adjusted automatically and it chooses a different learning rate for each parameter.\n",
        "\n",
        ">RMSProp divides the learning rate by the average of the exponential decay of squared gradients\n",
        "\n",
        "\n",
        "### Algorithm \n",
        "1. $  \\textbf{s} \\leftarrow  \\beta \\textbf{s} +  (1 - \\beta)\\triangledown_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\otimes  \\triangledown_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})$\n",
        "\n",
        "2. $ \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}- \\eta \\triangledown_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})   \\oslash \\sqrt{\\textbf{s} + \\epsilon}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BifL2RdF0cnt"
      },
      "source": [
        "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nrZ4XBE0cnu"
      },
      "source": [
        "## Adam\n",
        "\n",
        "**Adaptive Moment Estimation (Adam)** is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop.\n",
        "\n",
        ">Adam also keeps an exponentially decaying average of past gradients, similar to momentum.\n",
        "\n",
        ">Adam can be viewed as a combination of Adagrad and RMSprop,(Adagrad) which works well on sparse gradients and (RMSProp) which works well in online and nonstationary settings repectively.\n",
        "\n",
        ">Adam implements the **exponential moving average of the gradients** to scale the learning rate instead of a simple average as in Adagrad. It keeps an exponentially decaying average of past gradients.\n",
        "\n",
        ">Adam is computationally efficient and has very less memory requirement.\n",
        "\n",
        ">Adam optimizer is one of the most popular and famous gradient descent optimization algorithms.\n",
        "\n",
        "\n",
        "### Algorithm\n",
        "\n",
        "1. $\\textbf{m} \\leftarrow \\beta_1 \\textbf{m} + (1 - \\beta_1)\\triangledown_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})$\n",
        "\n",
        "2. $\\textbf{s} \\leftarrow  \\beta_2 \\textbf{s} +  (1 - \\beta_2)\\triangledown_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\otimes  \\triangledown_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})$\n",
        "\n",
        "3. $\\hat{\\textbf{m}} \\leftarrow \\frac{\\textbf{m}}{1 - \\beta_1^t}$\n",
        "\n",
        "4. $\\hat{\\textbf{s}} \\leftarrow \\frac{\\textbf{s}}{1 - \\beta_2^t}$\n",
        "\n",
        "5. $ \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}- \\eta \\hat{\\textbf{m}}  \\oslash \\sqrt{\\hat{\\textbf{s}} + \\epsilon}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAv0SQXq0cnu"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGnt8jCI0cnv"
      },
      "source": [
        "<!-- ## Comparisions -->\n",
        "\n",
        "<!-- ![alt](https://ml-cheatsheet.readthedocs.io/en/latest/_images/optimizers.gif)\n",
        "\n",
        "**<center>Figure :- SGD optimization on loss surface contours</center>**\n",
        "\n",
        "![alt](https://miro.medium.com/max/1628/1*SjtKOauOXFVjWRR7iCtHiA.gif)\n",
        "\n",
        "**<center>Figure :- SGD optimization on saddle point</center>** -->\n",
        "\n",
        "\n",
        "\n",
        "# How to choose optimizers?\n",
        "\n",
        "- If the data is sparse, use the self-applicable methods, namely Adagrad, Adadelta, RMSprop, Adam.\n",
        "\n",
        "- RMSprop, Adadelta, Adam have similar effects in many cases.\n",
        "\n",
        "- Adam just added bias-correction and momentum on the basis of RMSprop,\n",
        "\n",
        "- As the gradient becomes sparse, Adam will perform better than RMSprop.\n",
        "\n",
        "**Overall, Adam is the best choice.**\n",
        "\n",
        ">SGD is used in many papers, without momentum, etc. Although SGD can reach a minimum value, it takes longer than other algorithms and may be trapped in the saddle point.\n",
        "\n",
        "- If faster convergence is needed, or deeper and more complex neural networks are trained, an adaptive algorithm is needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMaQmt_z0cnv"
      },
      "source": [
        "Optimizer | Convergence Speed | Convergence quality\n",
        ":-:|:-:|:-:\n",
        "SGD | * | ***\n",
        "momentum | ** | ***\n",
        "NAG | ** | ***\n",
        "Adagrad | *** | * (stops too early)\n",
        "RMSprop | *** | ** to ***\n",
        "Adam | *** | ** to ***\n",
        "Nadam | *** | ** to ***\n",
        "AdaMax | *** | ** to ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdhZ5mXDRJY6"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVZGX9vZJVYw"
      },
      "source": [
        "Let's train a neural network on Fashion MNIST using the Leaky ReLU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFV1JjZdJVYw"
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8IKWEjkJVYw"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "LAYERS = [ tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.LeakyReLU(),\n",
        "    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.LeakyReLU(),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")]\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential(LAYERS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHVxFkNJJVYw"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbVQZkWNUUBZ",
        "outputId": "0a376e81-b309-49e5-8d3c-dabd21b88477"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5rKJye3JVYw",
        "scrolled": true,
        "outputId": "3f0f9360-e2d1-4840-c281-344c229d5118"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid), verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 - 4s - loss: 0.4943 - accuracy: 0.8208 - val_loss: 0.3800 - val_accuracy: 0.8670\n",
            "Epoch 2/10\n",
            "1719/1719 - 4s - loss: 0.3834 - accuracy: 0.8595 - val_loss: 0.4006 - val_accuracy: 0.8586\n",
            "Epoch 3/10\n",
            "1719/1719 - 4s - loss: 0.3547 - accuracy: 0.8682 - val_loss: 0.3400 - val_accuracy: 0.8798\n",
            "Epoch 4/10\n",
            "1719/1719 - 5s - loss: 0.3328 - accuracy: 0.8777 - val_loss: 0.3331 - val_accuracy: 0.8806\n",
            "Epoch 5/10\n",
            "1719/1719 - 5s - loss: 0.3167 - accuracy: 0.8824 - val_loss: 0.3222 - val_accuracy: 0.8808\n",
            "Epoch 6/10\n",
            "1719/1719 - 5s - loss: 0.3037 - accuracy: 0.8886 - val_loss: 0.3530 - val_accuracy: 0.8778\n",
            "Epoch 7/10\n",
            "1719/1719 - 5s - loss: 0.2954 - accuracy: 0.8911 - val_loss: 0.3342 - val_accuracy: 0.8808\n",
            "Epoch 8/10\n",
            "1719/1719 - 5s - loss: 0.2829 - accuracy: 0.8939 - val_loss: 0.3497 - val_accuracy: 0.8812\n",
            "Epoch 9/10\n",
            "1719/1719 - 5s - loss: 0.2751 - accuracy: 0.8977 - val_loss: 0.3367 - val_accuracy: 0.8860\n",
            "Epoch 10/10\n",
            "1719/1719 - 5s - loss: 0.2638 - accuracy: 0.9011 - val_loss: 0.3186 - val_accuracy: 0.8906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMXHHq_g0cnx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}