{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, We're in a situation to create a Sentiment analysis model, we have the dataset available but the problem here is machine will not going to understand the sentences of any languages, we have to clean those dataset by using stopwords, deleting punctuation and many more irrelevant things inside the data and We have to make it upto that level where we can feed those data to our machine or deep learning algorithms from that we can get some output with it.\n",
    "\n",
    "We are assuming you have a knowledge of Python, and if not, nothing to worry, we will give you some overview of it.\n",
    "\n",
    "## 1.Python String\n",
    "\n",
    "A String is like a sequence of Characters.\n",
    "\n",
    "A character is just a symbol.Like, the English language has 26 characters.\n",
    "\n",
    "Computers don't deal with characters,they deal with binary(numbers) only. Even though you seen characters but internally it is stored and manipulated with the combination of 0's and 1's.\n",
    "The conversion of character to a number is known as encoding, and the reverse is decoding.\n",
    "\n",
    "String literally surrounded by a single or double quotations like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iNeuron\n"
     ]
    }
   ],
   "source": [
    "a = \"iNeuron\"\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings are arrays\n",
    "\n",
    "Like in other popular programming languages, strings in Python are arrays of bytes represents unicode characters.\n",
    "\n",
    "However, Python doesn't have the character as the datatype, single character is just simply a string with length of 1.\n",
    "\n",
    "We used square bracket to access elements from the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r\n"
     ]
    }
   ],
   "source": [
    "#Get the character at position 4(Here, indexing starts from 0)\n",
    "a = \"iNeuron\"\n",
    "print(a[4])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "You can get the output upto certain range of characters by using the slicing index.\n",
    "\n",
    "Specify the start index and end index, which is separated by colon, to return a part of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uro\n",
      "--------------------------------------------------\n",
      "Neur\n",
      "--------------------------------------------------\n",
      "er\n"
     ]
    }
   ],
   "source": [
    "#To get the output from the position 3 to 6(not included)\n",
    "a = \"iNeuron\"\n",
    "print(a[3:6])\n",
    "print(\"-\" * 50)\n",
    "#To get the output by negative indexing from -6 position to -2 position.\n",
    "print(a[-6:-2])\n",
    "print(\"-\" * 50)\n",
    "#Get the results from position 2 to 6 but give result with the increment of 2.\n",
    "print(a[2:6:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Methods \n",
    "\n",
    "Python have set of built-in methods that you can use on strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iNeuron']\n"
     ]
    }
   ],
   "source": [
    "#strip() will remove whitespace in the string from begining to the end.\n",
    "a = \" iNeuron \"\n",
    "print(a.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ineuron\n",
      "INEURON\n"
     ]
    }
   ],
   "source": [
    "#lower() will lowercase the words which are upper in the sentences.\n",
    "a = \"iNeuron\"\n",
    "print(a.lower())\n",
    "\n",
    "#upper() will transform lowercase into upper.\n",
    "print(a.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meNeuron\n"
     ]
    }
   ],
   "source": [
    "#replace() will work like replace one string with another string.\n",
    "a = \"iNeuron\"\n",
    "print(a.replace(\"i\", \"me\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iNeur', 'n']\n"
     ]
    }
   ],
   "source": [
    "#split() will split the strings into substrings if it finds any instances of seprator.\n",
    "a = \"iNeuron\"\n",
    "print(a.split(\"o\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Concatenation\n",
    "\n",
    "To concatenate or combine two strings by use of + operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iNeuron Data\n"
     ]
    }
   ],
   "source": [
    "a = \"iNeuron\"\n",
    "b = \"Data\"\n",
    "print(a +\" \"+ b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import text\n",
    "\n",
    "In NLP your dataset would be in .txt, .csv type of format, you need to import it and try to clean the irrelevant datas from the dataset.Here we'll going to understand Python file handling:create,open,read,append,write.\n",
    "\n",
    "### Create a text file \n",
    "With the use of python you can create the text files by using code,we have demostrated here how you can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1\n",
    "\n",
    "file = open(\"iNeuron.txt\", \"w+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we declared file as a variable to open a file named iNeuron.txt.Open takes two arguemnets, first one is for the file we want to open and second one represents some kind of permissions or operation we want to do into that file.\n",
    "- Here we taken \"w\" letter as an arguemnt, which indicates write and will create a file if it is not exist in the library.\n",
    "- That \"+\" signs indicate both read and write.\n",
    "- The other option beside \"w\" are, \"r\" for read, and \"a\" for append."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2\n",
    "\n",
    "for i in range(5):\n",
    "    file.write(\"Line number is %d\\r\\n\" % (i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have a loop that runs over a range of 5 numbers.\n",
    "- Here using the **Write** function for entering data into the file.\n",
    "- The Output we want to iterate in the file is \"Line number is\", which we already declared with write function and then percent d(display integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This will close the instance of the file iNeuron.txt stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is the result after the execution of code.\n",
    "<img src=\".\\Images\\8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append data to a file\n",
    "\n",
    "You can also append a new text inside the existing file or in a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1\n",
    "\n",
    "file = open(\"iNeuron.txt\", \"a+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again \"+\" sign is in the code which means if .txt file are not available, this plus sign will create a new file but here is not any requirement to create a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2\n",
    "\n",
    "for i in range(3):\n",
    "    file.write(\"Appending Line number %d\\r\\n\" %(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will write data into a file in append mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seen earlier this close function will close the instance of the file iNeuron.txt stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the files\n",
    "\n",
    "Not only you create .txt files in Python but you can also call .txt file in a \"read mode\"(r). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1. Open the file in read mode\n",
    "\n",
    "file = open(\"iNeuron.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2. Here we'll check, is our file is open or not, if yes we proceed\n",
    "\n",
    "if file.mode == 'r':\n",
    "    content = file.read() #We used file.read() for reading the file data and store it in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line number is 1\n",
      "\n",
      "Line number is 2\n",
      "\n",
      "Line number is 3\n",
      "\n",
      "Line number is 4\n",
      "\n",
      "Line number is 5\n",
      "\n",
      "Appending Line number 1\n",
      "\n",
      "Appending Line number 2\n",
      "\n",
      "Appending Line number 3\n",
      "\n",
      "\n",
      "Here is the output!\n"
     ]
    }
   ],
   "source": [
    "#Step3. Printing the file\n",
    "print(content)\n",
    "print(\"Here is the output!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files mode in Python\n",
    "\n",
    "<img src=\".\\Images\\9.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping with Python\n",
    "\n",
    "Suppose you have to pull large amount of data from websites and you want to fetch it as quickly as possible. How would you do it? Manually going to the website and collect those datas.It will be tedious work. So, \"web scrapping\" will help you out in this situation. Web scrapping just makes this job easier and faster.\n",
    "\n",
    "Here, we will do Web Scrapping with Python, starts with\n",
    "\n",
    "1. Why we do web scrapping?\n",
    "Web scrapping will be used to collect large amount of data from Websites.But why does someone have to collect such large amount of data from websites? To know about this, let's have to look at the applications of web scrapping:\n",
    "\n",
    "- __Price comaparison:__ Parsehub is providing such services to useweb scraping to collect data from some online shopping websites and use to comapre price of products from another.\n",
    "- __Gathering Emails:__ There are lots of companies that use emails as a medium for marketing, they use web scrapping to collect email id's and send bulk emails.\n",
    "- __Social media scrapping:__ Web scrapping is used to collect data from Social Media websites such as Twitter to find out what's trending in twitter.\n",
    "- __Research and Development:__ For reasearch purposes people do web scrapping to collect a large set of data(Statistics, General information, temperature,etc.) from websites, which are analyzed and used to carry out surveys or for R&D.\n",
    "\n",
    "\n",
    "2. What is Web scrapping and is it legal or not?\n",
    "Web scrapping is an automated to extract large amount of data from websites.And the websites data are unstructured most of the time.Web scrapping will help you out to collect those unstructured data and stored it in a structured form.There are different ways to scrape websites such as online services,APIs, or by writing your own code. Here, we'll see how to implementing the web scraping with python.\n",
    "\n",
    "Coming to the question, is scrapping legal or not? Some websites allow web scrapping and some not.To know whether website allows you to scrape it or not by website's \"robots.txt\" file. You can find this file just append \"/robots.txt\" to the URL that you want to scrape.Here, we're scrapping from Flipkart website.So, to see the \"robots.txt\" file, URL is www.flipkart.com/robots.txt. \n",
    "\n",
    "3. How does web scrapping work?\n",
    "\n",
    "When we run the code for web scraping, a request is sent to the URL that you have mentioned in the code. As a response to the request, the server send the data and allows you to read the HTML or XML page. Then our code will parses the HTML or XML page, find the data and extract it.\n",
    "\n",
    "To extract datas using web scraping with python, you need to follow these basic steps:\n",
    "\n",
    "  1.Find that URL that you mentioned in the code and want to scrape it.\n",
    "  2.Inspect the Page for scraping.\n",
    "  3.Find those data you want to extract.\n",
    "  4.Write the code for scrapping.\n",
    "  5.Run the code and extract the data.\n",
    "  6.Store the data in the required format.\n",
    "  \n",
    "Now lets see how to extract data from the flipkart website using Python.\n",
    "\n",
    "4. Libraries used for Web scrapping\n",
    "\n",
    "We already know, that python used for various applications and there are different libraries for different purposes.In this, we're using the following libraries:\n",
    "\n",
    "- **Selenium:** Selenium library is used for web testing. We will use to automate browser activities.\n",
    "- **BeautifulSoup4:** It is generally used for parsing HTML and XML documents.It creates a parse trees that is helpful to extract the datas easily.\n",
    "- **Pandas:** It is a Python library used for data manipulation and analysis.Pandas is used to extract data and stored it in the desired format.\n",
    "\n",
    "5. For Demo Purpose : Scrapping a Flipkart Website\n",
    "\n",
    "Pre-requisites:\n",
    "\n",
    " - Python 3.x with Selenium, Beautifulsoup4, Pandas library  installed.\n",
    " - Google Chrome Browser\n",
    " \n",
    "You can go through this [link](https://github.com/iNeuronai/webscrappper_text.git) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Supose we have textual data available, we need to apply many of pre-processing steps to the data to transform those words into numerical features that work with machine learning algorithms.\n",
    "\n",
    "The pre-processing steps for the problem depend mainly on the domain and the problem itself.We don't need to apply all the steps for every problem.\n",
    "\n",
    "Here, we're going to see text preprocessing in Python. We'll use NLTK(Natural language toolkit) library here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries \n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text lowercase\n",
    "\n",
    "We do lowercase the text to reduce the size of the vocabulary of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather is too cloudy.possiblity of rain is high,today!!'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercase_text(text): \n",
    "    return text.lower() \n",
    "  \n",
    "input_str = \"Weather is too Cloudy.Possiblity of Rain is High,Today!!\"\n",
    "lowercase_text(input_str) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numbers\n",
    "\n",
    "We should either remove the numbers or convert those numbers into textual representations.\n",
    "We use regular expressions(re) to remove the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You bought  candies from shop, and  candies are in home.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Removing numbers \n",
    "def remove_num(text): \n",
    "    result = re.sub(r'\\d+', '', text) \n",
    "    return result \n",
    "  \n",
    "input_s = \"You bought 6 candies from shop, and 4 candies are in home.\"\n",
    "remove_num(input_s) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned above,you can also convert the numbers into words. This could be done by using the inflect library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You bought six candies from shop, and four candies are in home.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the library \n",
    "import inflect \n",
    "q = inflect.engine() \n",
    "  \n",
    "# convert number into text \n",
    "def convert_num(text): \n",
    "    # split strings into list of texts \n",
    "    temp_string = text.split() \n",
    "    # initialise empty list \n",
    "    new_str = [] \n",
    "  \n",
    "    for word in temp_string: \n",
    "        # if text is a digit, convert the digit \n",
    "        # to numbers and append into the new_str list \n",
    "        if word.isdigit(): \n",
    "            temp = q.number_to_words(word) \n",
    "            new_str.append(temp) \n",
    "  \n",
    "        # append the texts as it is \n",
    "        else: \n",
    "            new_str.append(word) \n",
    "  \n",
    "    # join the texts of new_str to form a string \n",
    "    temp_str = ' '.join(new_str) \n",
    "    return temp_str \n",
    "  \n",
    "input_str = 'You bought 6 candies from shop, and 4 candies are in home.'\n",
    "convert_num(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation\n",
    "\n",
    "We remove punctuations because of that we don't have different form of the same word. If we don't remove punctuations, then been, been, and been! will be treated separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey Are you excited After a week we will be in Shimla'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's remove punctuation \n",
    "def rem_punct(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator) \n",
    "  \n",
    "input_str = \"Hey, Are you excited??, After a week, we will be in Shimla!!!\"\n",
    "rem_punct(input_str) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove default stopwords:\n",
    "\n",
    "Stopwords are words that do not contribute to the meaning of the sentence. Hence, they can be safely removed without causing any change in the meaning of a sentence. The NLTK(Natural Language Toolkit) library has the set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data', 'new', 'oil', '.', 'A.I', 'last', 'invention']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing nltk library\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "# remove stopwords function \n",
    "def rem_stopwords(text): \n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    return filtered_text \n",
    "  \n",
    "ex_text = \"Data is the new oil. A.I is the last invention\"\n",
    "rem_stopwords(ex_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "From Stemming we will process of getting the root form of a word. Root or Stem is the part to which inflextional affixes(like -ed, -ize, etc) are added. We would create the stem words by removing the prefix of suffix of a word. So, stemming a word may not result in actual words.\n",
    "\n",
    "For Example: Mangoes ---> Mango\n",
    "\n",
    "             Boys ---> Boy\n",
    "             \n",
    "             going ---> go\n",
    "             \n",
    "             \n",
    "If our sentences are not in tokens, then we need to convert it into tokens. After we converted strings of text into tokens, then we can convert those word tokens into their root form. These are the Porter stemmer, the snowball stemmer, and the Lancaster Stemmer. We usually use Porter stemmer among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'is',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolut',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individu',\n",
       " 'would',\n",
       " 'gener',\n",
       " 'terabyt',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing nltk's porter stemmer \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "stem1 = PorterStemmer() \n",
    "  \n",
    "# stem words in the list of tokenised words \n",
    "def s_words(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stem1.stem(word) for word in word_tokens] \n",
    "    return stems \n",
    "  \n",
    "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
    "s_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "As stemming, lemmatization do the same but the only difference is that lemmatization ensures that root word belongs to the language. Because of the use of lemmatization we will get the valid words. In NLTK(Natural language Toolkit), we use WordLemmatizer to get the lemmas of words. We also need to provide a context for the lemmatization.So, we added pos(parts-of-speech) as a parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\paul\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\paul\\anaconda3\\lib\\site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: regex in c:\\users\\paul\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\paul\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\paul\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Paul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'be',\n",
       " 'the',\n",
       " 'new',\n",
       " 'revolution',\n",
       " 'in',\n",
       " 'the',\n",
       " 'World',\n",
       " ',',\n",
       " 'in',\n",
       " 'a',\n",
       " 'day',\n",
       " 'one',\n",
       " 'individual',\n",
       " 'would',\n",
       " 'generate',\n",
       " 'terabytes',\n",
       " 'of',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing nltk's WordNetlemmatizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize \n",
    "lemma = WordNetLemmatizer() \n",
    "# lemmatize string \n",
    "def lemmatize_word(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    # provide context i.e. part-of-speech(pos)\n",
    "    lemmas = [lemma.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    return lemmas \n",
    "  \n",
    "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
    "lemmatize_word(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech (POS) Tagging\n",
    "\n",
    "The pos(parts of speech) explain you how a word is used in a sentence. In the sentence, a word have different contexts and semantic meanings. The basic natural language processing(NLP) models like bag-of-words(bow) fails to identify these relation between the words. For that we use pos tagging to mark a word to its pos tag based on its context in the data. Pos is also used to extract rlationship between the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Are', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('afraid', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing tokenize library\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag \n",
    "  \n",
    "# convert text into word_tokens with their tags \n",
    "def pos_tagg(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    return pos_tag(word_tokens) \n",
    "  \n",
    "pos_tagg('Are you afraid of something?') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example NNP stands for Proper noun, PRP stands for personal noun, IN as Preposition. We can get all the details pos tags using the Penn Treebank tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n"
     ]
    }
   ],
   "source": [
    "# downloading the tagset  \n",
    "nltk.download('tagsets') \n",
    "  \n",
    "# extract information about the tag \n",
    "nltk.help.upenn_tagset('PRP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "Chunking is the process of extracting phrases from the Unstructured text and give them more structure to it. We also called them shallow parsing.We can do it on top of pos tagging. It groups words into chunks mainly for noun phrases. chunking we do by using regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP the/DT little/JJ red/JJ parrot/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk import pos_tag \n",
    "  \n",
    "# here we define chunking function with text and regular \n",
    "# expressions representing grammar as parameter \n",
    "def chunking(text, grammar): \n",
    "    word_tokens = word_tokenize(text) \n",
    "  \n",
    "    # label words with pos \n",
    "    word_pos = pos_tag(word_tokens) \n",
    "  \n",
    "    # create chunk parser using grammar \n",
    "    chunkParser = nltk.RegexpParser(grammar) \n",
    "  \n",
    "    # test it on the list of word tokens with tagged pos \n",
    "    tree = chunkParser.parse(word_pos) \n",
    "      \n",
    "    for subtree in tree.subtrees(): \n",
    "        print(subtree) \n",
    "    tree.draw() \n",
    "      \n",
    "sentence = 'the little red parrot is flying in the sky'\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunking(sentence, grammar) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we defined the grammar by using the regular expression rule. This rule tells you that NP(noun phrase) chunk should be formed whenever the chunker find the optional determiner(DJ) followed by any no. of adjectives and then a NN(noun).\n",
    "\n",
    "Image after running above code.\n",
    "<img src=\".\\Images\\11.png\">\n",
    "\n",
    "Libraries like Spacy and TextBlob are best for chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "It is used to extract information from unstructured text. It is used to classy the entities which is present in the text into categories like a person, organization, event, places, etc. This will give you a detail knowledge about the text and the relationship between the different entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Brain/NNP)\n",
      "  (PERSON Lara/NNP)\n",
      "  scored/VBD\n",
      "  the/DT\n",
      "  highest/JJS\n",
      "  400/CD\n",
      "  runs/NNS\n",
      "  in/IN\n",
      "  a/DT\n",
      "  test/NN\n",
      "  match/NN\n",
      "  which/WDT\n",
      "  played/VBD\n",
      "  in/IN\n",
      "  between/IN\n",
      "  (ORGANIZATION WI/NNP)\n",
      "  and/CC\n",
      "  (GPE England/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#Importing tokenization and chunk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag, ne_chunk \n",
    "  \n",
    "def ner(text): \n",
    "    # tokenize the text \n",
    "    word_tokens = word_tokenize(text) \n",
    "  \n",
    "    # pos tagging of words \n",
    "    word_pos = pos_tag(word_tokens) \n",
    "  \n",
    "    # tree of word entities \n",
    "    print(ne_chunk(word_pos)) \n",
    "  \n",
    "text = 'Brain Lara scored the highest 400 runs in a test match which played in between WI and England.'\n",
    "ner(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Regex\n",
    "\n",
    "As you're a software developer, you have probably encountered regular expressions many times and got consufed many times with these daunting set of characters grouped together like this:\n",
    "\n",
    "<img src=\".\\Images\\12.png\">\n",
    "\n",
    "And you may wondered what this is all about?\n",
    "\n",
    "Regular Expressions(Regx or RegExp) are too useful in stepping up your algorithm game and this will make you a better problem solver. The structure of Regx can be intimidating at first, but it is very rewarding once you got all the patterns and implement them in your work properly.\n",
    "\n",
    "\n",
    "## What is RegEx and why is it important?\n",
    "\n",
    "A Regex or we called it as regular expression, it is a type of object will help you out to extract information from any string data by searching through text and find it out what you need.Whether it's punctuation, numbers, letters, or even white spaces, RegEx will allow you to check and match any of the character combination in strings.\n",
    "\n",
    "For example, suppose you need to match the format of a email addresses or security numbers. You can utilize RegEx to check the pattern inside the text strings and use it to replace another substring.\n",
    "\n",
    "For instance, a RegEx could tell the program to search for the specific text from the string and then to print out the output accordingly. Expressions can include Text matching, Repetition of words,Branching,pattern-composition.\n",
    "\n",
    "Python supports RegEx through libraries. In RegEx supports for various things like **Identifiers, Modifiers, and White Space.**\n",
    "<img src=\".\\Images\\13.png\">\n",
    "\n",
    "### RegEx Syntax\n",
    "\n",
    "    import re\n",
    "\n",
    "- *re* library in Python is used for string searching and manipulation.\n",
    "- We also used it frequently for web scraping.\n",
    "\n",
    "#### Example for w+ and ^ Expression\n",
    "\n",
    "- *^:* Here in this expression matches the start of a string.\n",
    "- *w+:* This expression matches for the alphanumeric characters from inside the string.\n",
    "\n",
    "Here, we will give one example of how you can use \"w+\" and \"^\" expressions in code. re.findall will cover in next parts,so just focus on the \"w+\" and \"^\" expression.\n",
    "\n",
    "Let's have an example \"iNeuron13, Data is a new fuel\", if we execute the code we will get \"iNeuron13\" as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iNeuron13']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sent = \"iNeuron13, Data is a new fuel\"\n",
    "r2 = re.findall(r\"^\\w+\",sent)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* If we remove '+' sign from \\w, the output will change and it'll give only first character of the first letter, i.e [i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Example of \\s expression in re.split function\n",
    "\n",
    "- \"s:\" This expression we use for creating a space in the string.\n",
    "\n",
    "To understand better this expression we will use the split function in a simple example. In this example, we have to split each words using the \"re.split\" function and at the same time we have used \\s that allows to parse each word in the string seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'splited', 'this', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print((re.split(r'\\s','We splited this sentence')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above we got the output ['We', 'splited', 'this', 'sentence'] but what if we remove ' \\ ' from '\\s', it will give result like remove 's' from the entire sentences. Let's see in below example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We ', 'plited thi', ' ', 'entence']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print((re.split(r's','We splited this sentence')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, there are series of regular expression in Python that you can use in various ways like  \\d,\\D,$,\\.,\\b, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use RegEx methods\n",
    "\n",
    "The \"re\" packages provide several methods to actually perform queries on an input string. We will see different methods which are\n",
    "\n",
    "    re.match()\n",
    "    re.search()\n",
    "    re.findall()\n",
    "    \n",
    "**Note:** Based on the RegEx, Python offers two different primitive operations. This match method checks for the match only at the begining of the string while search checks for a match anywhere in the string.\n",
    "\n",
    "### Using re.match()\n",
    "\n",
    "The match function is used to match the RegEx pattern to string with optional flag. Here, in this \"w+\" and \"\\W\" will match the words starting from \"i\" and thereafter ,anything which is not started with \"i\" is not identified. For checking match for each element in the list or string, we run the for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('icecream', 'images')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "lists = ['icecream images', 'i immitated', 'inner peace']\n",
    "\n",
    "for i in lists:\n",
    "    q = re.match(\"(i\\w+)\\W(i\\w+)\", i)\n",
    "    \n",
    "    if q:\n",
    "        print((q.groups()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Pattern in the text(re.search())\n",
    "\n",
    "A RegEx is commonly used to search for a pattern in the text. This method takes a RegEx pattern and a string and searches that pattern with the string.\n",
    "\n",
    "For using re.search() function, you need to import re first. The search() function takes the \"pattern\" and \"text\" to scan from our given string and returns the match object when the pattern found or else not match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're looking for '['playing', 'iNeuron']' in 'Raju is playing outside.' Found match!\n",
      "You're looking for '['playing', 'iNeuron']' in 'Raju is playing outside.' no match found!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = [\"playing\", \"iNeuron\"]\n",
    "text = \"Raju is playing outside.\"\n",
    "\n",
    "for p in pattern:\n",
    "    print(\"You're looking for '%s' in '%s'\" %(pattern, text), end = ' ')\n",
    "    \n",
    "    if re.search(p, text):\n",
    "        print('Found match!')\n",
    "        \n",
    "    else:\n",
    "        print(\"no match found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Above example, we look for two literal strings \"playing\", \"iNeuron\" and in text string we had taken \"Raju is playing outside.\". For \"playing\" we got the match and in the output we got \"Found Match\", while for the word \"iNeuron\" we didn't got any match. So,we got no match found for that word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using re.findall() for text\n",
    "\n",
    "We use re.findall() module is when you wnat to iterate over the lines of the file, it'll do like list all the matches in one go. Here in a example, we would like to fetch email address from the list and we want to fetch all emails from the list, we use re.findall() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaurav@iNeuron.ai\n",
      "Nilesh@iNeuron.ai\n",
      "Jay@iNeuron.ai\n",
      "Vikash@iNeuron.ai\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "kgf = \"Gaurav@iNeuron.ai, Nilesh@iNeuron.ai, Jay@iNeuron.ai, Vikash@iNeuron.ai\"\n",
    "\n",
    "emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', kgf)\n",
    "\n",
    "for e in emails:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Flags\n",
    "\n",
    "You see many Python RegEx methods and functions take an optional arguemnet flag.This flag can modify the meaning of the given regeEx pattern.\n",
    "\n",
    "Various flags used in python include.\n",
    "<img src=\".\\Images\\14.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look the example for re.M or Multiline Flags\n",
    "\n",
    "In the multiline flag the pattern character \"^\" matches the first character of the string and the begining of the each line. While the small \"w\" is used to mark the space with characters.When you run the code first variable \"q1\" prints out the character \"i\" only and while using the Multiline flag will give the result of all first character of all strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i']\n",
      "['i', 'M', 'L']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "aa = \"\"\"iNeuron13\n",
    "Machine\n",
    "Learning\"\"\"\n",
    "\n",
    "q1 = re.findall(r\"^\\w\", aa)\n",
    "q2 = re.findall(r\"^\\w\", aa, re.MULTILINE)\n",
    "print(q1)\n",
    "print(q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, you can also use other Python flags like re.U (Unicode), re.L (Follow locale), re.X (Allow Comment), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text normalization\n",
    "\n",
    "In the tect pre-processing highly overlooked step is text normalization. The text normalization means the process of transforming the text into the canonical(or standard) form. Like, \"ok\" and \"k\" can be transformed to \"okay\", its canonical form.And another example is mapping of near identical words such as \"preprocessing\", \"pre-processing\" and \"pre processing\" to just \"preprocessing\".\n",
    "\n",
    "Text normaliztion is too useful for noisy textssuch as social media comments, comment to blog posts, text messages, where abbreviations, misspellings, and the use out-of-vocabulary(oov) are prevalent.\n",
    "\n",
    "<img src=\".\\Images\\15.png\">\n",
    "\n",
    "### Effects of normalization\n",
    "\n",
    "Text normalization has even been effective for analyzing highly unstructured clinical texts where physicians take notes in non-standard ways. We have also found it useful for topic extraction where near synonyms and spelling differences are common (like 'topic modelling', 'topic modeling', 'topic-modeling', 'topic-modelling').\n",
    "\n",
    "Unlike stemming and lemmatization, there is not a standard way to normalize texts. It typically depends on the task. For e.g, the way you would normalize clinical texts would arguably be different from how you normalize text messages.\n",
    "\n",
    "Some of the common approaches to text normalization include dictionary mappings, statistical machine translation (SMT) and spelling-correction based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count\n",
    "\n",
    "I am assuming you have the understanding of tokenization,the first figure we can calculate is the word frequency.By *word frequency* we can find out how many times each tokens appear in the text. When talking about word frequency, we distinguished between *types* and *tokens*.Types are the distinct words in a corpus, whereas tokens are the words, including repeats. Let's see how this works in practice.\n",
    "\n",
    "Let's take an example for better understanding:\n",
    "\n",
    "“There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,” the Prime Minister tweeted.\n",
    "\n",
    "How many tokens and types are there in above sentences?\n",
    "\n",
    "Let's use Python for calculating these figures. First, tokenize the sentence by using the tokenizer which uses the non-alphabetic characters as a separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.regexp import WhitespaceTokenizer\n",
    "m = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the above we had used a slightly different syntax for importing the module. You'll recognize by now the variable assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "tokens = WhitespaceTokenizer().tokenize(m)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "my_vocab = set(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to perform the same operation but with the different tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_st = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import different tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.regexp import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above tokenizer also split the words into tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "m_t = WordPunctTokenizer().tokenize(my_st)\n",
    "\n",
    "print(len(m_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "my_vocab = set(m_t)\n",
    "print(len(my_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between the above approaches? In the first one, vocabulary ends up containing \"words\" and \"words.\" as two distinct words; whereas in second example \"words\" is a token type and \".\" (i.e. the dot) is split into a separate token and this results into a new token type in addition to \"words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency distribution\n",
    "\n",
    "What is Frequency distribution? This is basically counting words in your texts.To give a brief example of how it works,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<FreqDist with 19317 samples and 260819 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    " \n",
    "print(\"\\n\\n\\n\")\n",
    "freqDist = FreqDist(text1)\n",
    "print(freqDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class **FreqDist** works like a dictionary where keys are the words in the text and the values are count associated with that word. For example, if you want to see how many words \"person\" are in the text, you can type as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(freqDist[\"person\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important function in **FreqDist** is the **.keys()** function. Let us see what will it give in a below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict_keys'>\n"
     ]
    }
   ],
   "source": [
    "words = freqDist.keys()\n",
    "print(type(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running above code, it'll give as class 'dict_keys', in the other words, you get a list of all the words in your text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you want to see how many words are there in the text,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19317\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the class nltk.text.Text function do the same stuff, so what is the difference? The difference is that with FreqDist you can create your own texts without the necessity of converting your text to nltk.text.Text class.\n",
    "\n",
    "And the other usual functon is *plot*. Plot will do like it displays the most used words in your text. So, if you want to see 15 most used words in the text , For example like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x262dd95dcc8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqDist.plot(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Frequency Distribution\n",
    "\n",
    "Suppose you want to do frequency distribution based on your own personal text. Let's get started,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    " \n",
    "sent = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"\n",
    " \n",
    "text_list = sent.split(\" \")\n",
    " \n",
    "freqDist = FreqDist(text_list)\n",
    "words = list(freqDist.keys())\n",
    " \n",
    "print(freqDist['need'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first line, you don't have to import nltk.book to use the FreqDist class.\n",
    "\n",
    "We then declare sent and text_list variables. The variable *sent* is your custom text and the variable text_list is a list that contains all the words of your custom text.You can see that we used sent.split(\" \") to separate the words.\n",
    "\n",
    "Then you have the variable *freqDist* and words. freqDist is an object of the FreqDist class is for the text you have given and words is the list of all keys of freqDist.\n",
    "\n",
    "The last line of code is where you print your results. In this example, your code will print the count of the word “need”.\n",
    "\n",
    "If you replace “need” with “Prime”, you can see that it will return 1 instead of 2. This is because nltk indexing is case-sensitive. To avoid this, you can use the *.lower()* function in the variable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
