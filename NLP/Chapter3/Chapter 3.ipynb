{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob: Simplified Text Processing \n",
    "\n",
    "TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation,and more.\n",
    "\n",
    "TextBlob stands on the giant shoulders of NLTK and pattern, and plays nicely with both.\n",
    "\n",
    "## Features\n",
    "- Noun phrase extraction\n",
    "- Part-of-speech tagging\n",
    "- Sentiment analysis\n",
    "- Classification (Naive Bayes, Decision Tree)\n",
    "- Language translation and detection powered by Google Translate\n",
    "- Tokenization (splitting text into words and sentences)\n",
    "- Word and phrase frequencies\n",
    "- Parsing\n",
    "- n-grams\n",
    "- Word inflection (pluralization and singularization) and lemmatization\n",
    "- Spelling correction\n",
    "- Add new models or languages through extensions\n",
    "- WordNet integration\n",
    "\n",
    "## Installation\n",
    "\n",
    "### Installing/Uploading from the PyPi\n",
    "     \n",
    "      pip install textblob\n",
    "      python -m textblob.download_corpora\n",
    "\n",
    "This will install TextBlob and download the necessary NLTK corpora. If you need to change the default download directory set the NLTK_DATA environment variable.\n",
    "\n",
    "### Downloading the minimum corpora:\n",
    "\n",
    "If you only intend to use TextBlob’s default models (no model overrides), you can pass the lite argument. This downloads only those corpora needed for basic functionality.\n",
    "\n",
    "      python -m textblob.download_corpora lite\n",
    "      \n",
    "### Installing with Conda\n",
    "\n",
    "***Note:*** Conda builds are currently available for Mac OSX only.\n",
    "\n",
    "TextBlob is also available as a conda package. To install with conda, run\n",
    "\n",
    "     conda install -c https://conda.anaconda.org/sloria textblob\n",
    "     python -m textblob.download_corpora\n",
    "     \n",
    "### Python\n",
    "\n",
    "TextBlob supports Python >=2.7 or >=3.4.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "TextBlob depends on NLTK3. NLTK will be installed automatically when you run\n",
    "     \n",
    "     pip install textblob\n",
    "     \n",
    "     \n",
    "TextBlob aims to provide access to common text-processing operations through a familiar interface. You can treat **TextBlob** objects as if they were Python strings that learned how to do Natural Language Processing.\n",
    "\n",
    "### Create a TextBlob\n",
    "\n",
    "First, the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our first **TextBlob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = TextBlob(\"I love Natural Language Processing, not you!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech(POS) Tagging\n",
    "\n",
    "Parts-of-speech tags can be accessed through the **tags** property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('love', 'VBP'),\n",
       " ('Natural', 'JJ'),\n",
       " ('Language', 'NNP'),\n",
       " ('Processing', 'NNP'),\n",
       " ('not', 'RB'),\n",
       " ('you', 'PRP')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Phrase Extraction\n",
    "\n",
    "Similarly, noun phrases are accessed through the **noun_phrases** property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['language processing'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "The sentiment property returns a named tuple of the form Sentiment(polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.39166666666666666, subjectivity=0.4357142857142857)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n",
    "testimonial.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4357142857142857"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testimonial.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Data', 'is', 'a', 'new', 'fuel', 'Explicit', 'is', 'better', 'than', 'implicit', 'Simple', 'is', 'better', 'than', 'complex'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen = TextBlob(\"Data is a new fuel. \"\n",
    "               \"Explicit is better than implicit. \"\n",
    "               \"Simple is better than complex. \")\n",
    "               \n",
    "zen.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Data is a new fuel.\"),\n",
       " Sentence(\"Explicit is better than implicit.\"),\n",
       " Sentence(\"Simple is better than complex.\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sentences* objects have the same properties and methods as ***TextBlobs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is a new fuel.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n"
     ]
    }
   ],
   "source": [
    "for sentence in zen.sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Inflection and lemmatization\n",
    "\n",
    "Each word in the **TextBlob.words** or **Sentence.words** is a **Word** object(a subclass of unicode) with useful methods, e.g. for word inflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Use', '4', 'spaces', 'per', 'indentation', 'level'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = TextBlob('Use 4 spaces per indentation level')\n",
    "\n",
    "sentence.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'space'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.words[2].singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Uses'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.words[0].pluralize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words can be lemmatized just by calling the **lemmatize** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lion'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "\n",
    "q = Word('lions')\n",
    "q.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = Word(\"went\")\n",
    "q.lemmatize(\"v\") #Pass in WordNet part of speech (verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet Integeration\n",
    "\n",
    "You can access the synets for a **Word** via the **synsets** property or the **get_synsets** method optionally passing in a parts-of-speech.\n",
    "\n",
    "### WordNet \n",
    "\n",
    "   WordNet is a lexical database that is dictionary for the English language, it is specifically for the natural language     processing.\n",
    "### Synset\n",
    "\n",
    "   It is a special kind of a simple interface that is present in the NLTK for look up words in WordNet. Synset instances are    the groupings of synonymous that express the same type of concept. Some words have only one synset and some have several."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('goat.n.01'),\n",
       " Synset('butt.n.03'),\n",
       " Synset('capricorn.n.01'),\n",
       " Synset('capricorn.n.03')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "from textblob.wordnet import VERB\n",
    "word = Word(\"goat\")\n",
    "word.synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chop.v.05'),\n",
       " Synset('hack.v.02'),\n",
       " Synset('hack.v.03'),\n",
       " Synset('hack.v.04'),\n",
       " Synset('hack.v.05'),\n",
       " Synset('hack.v.06'),\n",
       " Synset('hack.v.07'),\n",
       " Synset('hack.v.08')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"hack\").get_synsets(pos=VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the definitions for each synset via the **definitions** property or the **define()** method, which can also take an optional part-of-speech(pos) argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the linear extent in space from one end to the other; the longest dimension of something that is fixed in place',\n",
       " 'continuance in time',\n",
       " 'the property of being the extent of something from beginning to end',\n",
       " 'size of the gap between two places',\n",
       " 'a section of something that is long and narrow']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"length\").definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create synsets directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob.wordnet import Synset\n",
    "octopus = Synset('octopus.n.02')\n",
    "shrimp = Synset('shrimp.n.03')\n",
    "octopus.path_similarity(shrimp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordLists\n",
    "\n",
    "A wordlist is just the Python list with additional methods.\n",
    "\n",
    "WordLists will find it out the words which are in the sentence and ignore the spaces in between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['cow', 'sheep', 'octopus'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals = TextBlob(\"cow sheep octopus\")\n",
    "animals.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['kine', 'sheep', 'octopodes'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.words.pluralize() # It'll pluralize the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction\n",
    "\n",
    "For correcting the words you can use **correct()** method to attempt spelling correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An you pronounce czechoslovakia?\n"
     ]
    }
   ],
   "source": [
    "g = TextBlob('Can you pronounce czechuslovakia?')\n",
    "print(g.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word objects have a **spellcheck() Word.spellcheck()** , this method that returns a list of (word, confidence) tuples with spelling suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('longitude', 1.0)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "k = Word('longituode')\n",
    "k.spellcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This spelling correction is based on the Peter Norvig's \"How to Write a Spelling Corrector\", as implemented in the pattern library. It is about 70% accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Word and Noun Phrase Frequencies\n",
    "\n",
    "There are two ways to get the frequency of a word or noun phrase in the **TextBlob**\n",
    "\n",
    "The first one is through the word_counts dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = TextBlob('She sales sea shells at the sea shore.')\n",
    "\n",
    "sent.word_counts['sea']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you access the frequencies this way, the search will not be case sensitive, and words that are not found will have a frequency of 0.\n",
    "\n",
    "The second way is to use the count() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.words.count('sea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify whether or not the search should be case-sensitive (default is False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.words.count('Sea', case_sensitive=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example we have given 'Sea' and ofcourse 'Sea' is not available in the sentence, 'Sea' is available in sentence but in lowercase beacause of that it given 0 as a result.\n",
    "\n",
    "Each of these methods can also be used with noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.noun_phrases.count('sea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation and Language Detection\n",
    "\n",
    "TextBlobs can be translated between languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"कुछ नहीं से कुछ भला।\")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(u'Something is better than nothing.')\n",
    "blob.translate(to='hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no source language is specified, **TextBlob** will attempt to detect the language. You can specify the source language explicitly, like so. Raises **TranslatorError** if the TextBlob cannot be translated into the requested language or NotTranslated if the translated result is the same as the input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Better than nothing\")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_blob = TextBlob(u\"有总比没有好\")\n",
    "chinese_blob.translate(from_lang=\"zh-CN\", to='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also attempt to detect a TextBlob’s language using **TextBlob.detect_language().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = TextBlob(\"कुछ नहीं से कुछ भला\")\n",
    "d.detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n",
    "\n",
    "Use the **parse()** method to parse the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And/CC/O/O now/RB/B-ADVP/O for/IN/B-PP/B-PNP something/NN/B-NP/I-PNP completely/RB/B-ADJP/O different/JJ/I-ADJP/O ././O/O\n"
     ]
    }
   ],
   "source": [
    "b = TextBlob(\"And now for something completely different.\")\n",
    "print(b.parse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, TextBlob uses pattern’s parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlobs Are Like Python Strings!\n",
    "\n",
    "You can use Python’s substring syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Data is a new f\")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it as common string method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"DATA IS A NEW FUEL. EXPLICIT IS BETTER THAN IMPLICIT. SIMPLE IS BETTER THAN COMPLEX. \")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.find('than') #It shows that 'than' word starts from 39th place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make comparisons between TextBlobs and strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_blob = TextBlob('apple')\n",
    "s_blob = TextBlob('samsumg')\n",
    "\n",
    "a_blob < s_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_blob == 'apple'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can concatenate and interpolate TextBlobs and strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"apple and samsumg\")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_blob + ' and ' + s_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple and samsumg'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{0} and {1}\".format(a_blob,s_blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams\n",
    "\n",
    "The **TextBlob.ngrams()** method returns a list of tuples of n successive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Now', 'is', 'better']),\n",
       " WordList(['is', 'better', 'than']),\n",
       " WordList(['better', 'than', 'never'])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"Now is better than never.\")\n",
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Start and End Indices of Sentences\n",
    "\n",
    "Use sentence.start and sentence.end to get the indices where a sentence starts and ends within a **TextBlob.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is a new fuel.\n",
      "---- Starts at index 0, Ends at index 19\n",
      "Explicit is better than implicit.\n",
      "---- Starts at index 20, Ends at index 53\n",
      "Simple is better than complex.\n",
      "---- Starts at index 54, Ends at index 84\n"
     ]
    }
   ],
   "source": [
    "for k in zen.sentences:\n",
    "    print(k)\n",
    "    print(\"---- Starts at index {}, Ends at index {}\".format(k.start, k.end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start building the Text Classification system\n",
    "\n",
    "The __textblob.classifiers__ module makes it simple to create custom classifiers.\n",
    "\n",
    "As an example, let’s create a custom sentiment analyzer.\n",
    "\n",
    "## Loading Data and Creating a Classifier\n",
    "\n",
    "First we’ll create some training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\n",
    "     ('I love this sandwich.', 'pos'),\n",
    "     ('this is an amazing place!', 'pos'),\n",
    "     ('I feel very good about these beers.', 'pos'),\n",
    "     ('this is my best work.', 'pos'),\n",
    "     (\"what an awesome view\", 'pos'),\n",
    "     ('I do not like this restaurant', 'neg'),\n",
    "     ('I am tired of this stuff.', 'neg'),\n",
    "     (\"I can't deal with this\", 'neg'),\n",
    "     ('he is my sworn enemy!', 'neg'),\n",
    "     ('my boss is horrible.', 'neg')\n",
    "]\n",
    "\n",
    "test = [\n",
    "     ('the beer was good.', 'pos'),\n",
    "     ('I do not enjoy my job', 'neg'),\n",
    "     (\"I ain't feeling dandy today.\", 'neg'),\n",
    "     (\"I feel amazing!\", 'pos'),\n",
    "     ('Gary is a friend of mine.', 'pos'),\n",
    "     (\"I can't believe I'm doing this.\", 'neg')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll create a Naive Bayes classifier, passing the training data into the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data from Files\n",
    "\n",
    "You can also load data from common file formats including CSV, JSON, and TSV.\n",
    "\n",
    "CSV files should be formatted like so:\n",
    "\n",
    "      I love this sandwich.,pos\n",
    "      This is an amazing place!,pos\n",
    "      I do not like this restaurant,neg\n",
    "      \n",
    "JSON files should be formatted like so:\n",
    "\n",
    "[\n",
    "    {\"text\": \"I love this sandwich.\", \"label\": \"pos\"},\n",
    "    {\"text\": \"This is an amazing place!\", \"label\": \"pos\"},\n",
    "    {\"text\": \"I do not like this restaurant\", \"label\": \"neg\"}\n",
    "]\n",
    "\n",
    "You can then pass the opened file into the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'r') as fp:\n",
    "    cl = NaiveBayesClassifier(fp, format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Text\n",
    "\n",
    "Call the *classify(text)* method to use the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.classify(\"This is an amazing library!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the label probability distribution with the *prob_classify(text)* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dist = cl.prob_classify(\"I am suffering from cough and cold.\")\n",
    "prob_dist.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(prob_dist.prob(\"neg\"), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(prob_dist.prob(\"pos\"), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying TextBlobs\n",
    "\n",
    "Another way to classify text is to pass a classifier into the constructor of TextBlob and call its *classify()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"Alcohol is good. But the hangover is horrible.\", classifier=cl)\n",
    "blob.classify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of this approach is that you can classify sentences within a **TextBlob**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol is good.\n",
      "pos\n",
      "But the hangover is horrible.\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "for b in blob.sentences:\n",
    "    print(b)\n",
    "    print(b.classify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifiers\n",
    "\n",
    "To compute the accuracy on our test set, use the **accuracy(test_data)** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the show_informative_features() method to display a listing of the most informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             contains(I) = False             pos : neg    =      1.9 : 1.0\n",
      "          contains(this) = False             pos : neg    =      1.9 : 1.0\n",
      "             contains(I) = True              neg : pos    =      1.7 : 1.0\n",
      "          contains(this) = True              neg : pos    =      1.7 : 1.0\n",
      "            contains(an) = False             neg : pos    =      1.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "cl.show_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Classifiers with New Data\n",
    "\n",
    "Use the update(new_data) method to update a classifier with new training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = [('She is my best friend.', 'pos'),\n",
    "           (\"I'm happy to have a new friend.\", 'pos'),\n",
    "           (\"Stay thirsty, my friend.\", 'pos'),             \n",
    "           (\"He ain't from around here.\", 'neg')]\n",
    "\n",
    "cl.update(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractors\n",
    "\n",
    "By default, the *NaiveBayesClassifier* uses a simple feature extractor that indicates which words in the training set are contained in a document.\n",
    "\n",
    "For example, the sentence “I love” might have the features contains(love): True or contains(hate): False.\n",
    "\n",
    "You can override this feature extractor by writing your own. A feature extractor is simply a function with document (the text to extract features from) as the first argument. The function may include a second argument, train_set (the training dataset), if necessary.\n",
    "\n",
    "The function should return a dictionary of features for document.\n",
    "\n",
    "For example, let’s create a feature extractor that just uses the first and last words of a document as its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_word_extractor(document):\n",
    "    tokens = document.split()\n",
    "    first_word, last_word = tokens[0], tokens[-1]\n",
    "    feats = {}\n",
    "    feats[\"first({0})\".format(first_word)] = True\n",
    "    feats[\"last({0})\".format(last_word)] = False\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = end_word_extractor(\"I love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert features == {'last(love)': False, 'first(I)': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the feature extractor in a classifier by passing it as the second argument of the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl2 = NaiveBayesClassifier(test, feature_extractor=end_word_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"I'm excited to try my new classifier.\", classifier=cl2)\n",
    "blob.classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: [link]('https://textblob.readthedocs.io/en/dev/classifiers.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
